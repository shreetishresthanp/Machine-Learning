{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fac98569",
      "metadata": {
        "id": "fac98569"
      },
      "source": [
        "# Homework 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31c6e87",
      "metadata": {
        "id": "e31c6e87"
      },
      "source": [
        "## 2. Recurrent Network (NN) Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fd76527a",
      "metadata": {
        "id": "fd76527a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"UCI HAR Dataset.zip\"\n",
        "# !unzip \"/content/human+activity+recognition+using+smartphones.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGwu1z0rWOwH",
        "outputId": "f577d036-755b-4a40-c85b-32ebcc2309eb"
      },
      "id": "lGwu1z0rWOwH",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCI HAR Dataset.zip\n",
            "replace UCI HAR Dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9dda9d3b",
      "metadata": {
        "id": "9dda9d3b"
      },
      "outputs": [],
      "source": [
        "# Helper function to load the raw inertial signals\n",
        "def load_ucihar(data_dir=\"UCI HAR Dataset\", subset=\"train\"):\n",
        "    \"\"\" Loads the UCI HAR data from the Inertial Signals folder. Returns: X: numpy array of shape (num_samples, seq_len, num_signals) y: numpy array of labels (0-indexed) \"\"\"\n",
        "    # The nine signal types available in the dataset\n",
        "    signal_types = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\" ]\n",
        "    signals = []\n",
        "    # Each signal file is located in {data_dir}/{subset}/Inertial Signals/\n",
        "    for signal in signal_types:\n",
        "        filename = os.path.join(data_dir, subset, \"Inertial Signals\", f\"{signal}_{ subset}.txt\")\n",
        "        # Each file has shape (num_samples, 128)\n",
        "        data = np.loadtxt(filename) # Add a new axis so that we can later stack to shape (num_samples, 128, num_signals)\n",
        "        signals.append(data[..., np.newaxis]) # Stack along the last dimension to form (num_samples, 128, 9)\n",
        "    X = np.concatenate(signals, axis=2) # Load labels from y_{subset}.txt; labels in the dataset are 1-indexed, so subtract 1.\n",
        "    y_path = os.path.join(data_dir, subset, f\"y_{subset}.txt\")\n",
        "    y = np.loadtxt(y_path).astype(int)- 1\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a PyTorch Dataset for UCI HAR\n",
        "class UCIHARDataset(Dataset):\n",
        "  def __init__(self, data_dir=\"UCI HAR Dataset\", subset=\"train\"):\n",
        "    self.X, self.y = load_ucihar(data_dir, subset)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "    label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "    return sample, label"
      ],
      "metadata": {
        "id": "8RwciBLNWICP"
      },
      "id": "8RwciBLNWICP",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "default_input_dim = 9\n",
        "default_feature_dim = 16\n",
        "default_num_classes = 6\n",
        "\n",
        "# define model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first= True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x. device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        logit = self.fc(out[:,-1, :])\n",
        "        prob = nn.functional.softmax(logit, dim=1)\n",
        "        return prob, logit\n",
        ""
      ],
      "metadata": {
        "id": "guVfjfwaXHeP"
      },
      "id": "guVfjfwaXHeP",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size, lr = 10, 16, 0.001\n",
        "\n",
        "# create train, test dataset\n",
        "train_dataset = UCIHARDataset(subset=\"train\")\n",
        "test_dataset = UCIHARDataset(subset=\"test\")\n",
        "\n",
        "#    create train, test loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle= True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle= False)\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=1, num_classes=6)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "U48stUVIvXUx"
      },
      "id": "U48stUVIvXUx",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
        "    # start training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            activity_prob, activity_logit = model(inputs)\n",
        "            loss = criterion(activity_logit, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on test data\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            activity_prob, activity_logit = model(inputs)\n",
        "            _, predicted = torch.max(activity_logit.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "6xMOnYKhvlxz"
      },
      "id": "6xMOnYKhvlxz",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-KVt_hXwpBn",
        "outputId": "f1af5a45-ff6a-403d-ac9b-56ecab5377c5"
      },
      "id": "N-KVt_hXwpBn",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim = 64\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=feature_dim, num_layers=1, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8ctji2-y-nk",
        "outputId": "07207df4-1e1d-4d6d-f3e8-a3039e19de6b"
      },
      "id": "_8ctji2-y-nk",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim = 16\n",
        "num_layers = 2\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=feature_dim, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzHwO4axy82g",
        "outputId": "bd7f3ec6-9280-4880-8efd-21f8a555ff27"
      },
      "id": "LzHwO4axy82g",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 3\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5XrGk4C1566",
        "outputId": "70a744c2-7629-48bc-d9ef-b4568ea1de63"
      },
      "id": "a5XrGk4C1566",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ep-rEO16uj",
        "outputId": "ce425aad-ca92-40d0-90ff-8cd7b2f2c438"
      },
      "id": "r9ep-rEO16uj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.2891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace RNN with LSTM (feature_dim=16, num_layers=1)\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        logit = self.fc(out[:, -1, :])\n",
        "        prob = nn.functional.softmax(logit, dim=1)\n",
        "        return prob, logit\n",
        "\n",
        "feature_dim = 16\n",
        "num_layers = 1\n",
        "input_size = 9\n",
        "num_classes = 6\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = LSTMClassifier(input_size=input_size, hidden_size=feature_dim, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w90xgqUg2FEu",
        "outputId": "0431f245-5462-4bab-9c19-e702ce1b49c1"
      },
      "id": "w90xgqUg2FEu",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "CAJAq6Tt6Gge"
      },
      "id": "CAJAq6Tt6Gge",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" Splits the image into patches and embeds them. \"\"\"\n",
        "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        # We use a simple conv layer to perform patchify + embedding in one step.\n",
        "        self.proj = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
        "        # Number of patches\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        # Class token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_size))\n",
        "        # Positional embedding\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, num_patches + 1, emb_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" x shape: (B, 3, 32, 32) returns: (B, N+1, emb_size) \"\"\"\n",
        "        B = x.size(0)\n",
        "        # Conv2d-> (B, emb_size, H’, W’), with H’ and W’ = 32 // patch_size\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Class token\n",
        "        cls_token = self.cls_token.expand(B,-1,-1) # (B, 1, emb_size)\n",
        "        x = torch.cat([cls_token, x], dim=1) # (B, N+1, emb_size)\n",
        "        # Add positional embedding\n",
        "        x = x + self.pos_emb[:, : x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "Spw-Nf-k6JII"
      },
      "id": "Spw-Nf-k6JII",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, emb_size=128, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = emb_size // num_heads\n",
        "        self.qkv = nn.Linear(emb_size, 3 * emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x): # x shape: (B, N, emb_size)\n",
        "        B, N, _ = x.shape\n",
        "        qkv = self.qkv(x) # (B, N, 3*emb_size)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4) # (3, B, num_heads, N, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2] # each: (B, num_heads, N, head_dim)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        # # scores shape: (B, num_heads, N, N)\n",
        "        scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
        "        att = torch.softmax(scores, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "\n",
        "        # out shape: (B, num_heads, N, head_dim)\n",
        "        out = torch.matmul(att, v)\n",
        "\n",
        "        # Combine heads\n",
        "        out = out.transpose(1, 2) # (B, N, num_heads, head_dim)\n",
        "        out = out.flatten(2) # (B, N, emb_size)\n",
        "        out = self.projection(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "q3Zu_EVI6MEF"
      },
      "id": "q3Zu_EVI6MEF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_size=128, num_heads=4, expansion=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.attn = MultiHeadSelfAttention(emb_size, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(expansion * emb_size, emb_size)\n",
        "            )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention block\n",
        "        x_res = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attn(x)\n",
        "        x = x_res + self.drop(x)\n",
        "\n",
        "        # Feed-forward block\n",
        "        x_res = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x_res + self.drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KhoWjlA76Pkx"
      },
      "id": "KhoWjlA76Pkx",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, in_channels=3, patch_size=4, emb_size=128, img_size=32, num_heads=4, num_layers=6, num_classes=10, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
        "        self.encoder = nn.Sequential(*[\n",
        "            TransformerEncoderBlock(\n",
        "                emb_size=emb_size,\n",
        "                num_heads=num_heads,\n",
        "                expansion=4,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.cls_head = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (B, 3, 32, 32)\n",
        "        x = self.patch_embed(x) # (B, N+1, emb_size)\n",
        "        x = self.encoder(x) # (B, N+1, emb_size)\n",
        "        x = self.norm(x) # (B, N+1, emb_size)\n",
        "\n",
        "        # The first token is the class token\n",
        "        cls_token_final = x[:, 0]\n",
        "        out = self.cls_head(cls_token_final) # (B, num_classes)\n",
        "        return out"
      ],
      "metadata": {
        "id": "vFwrDq8E6Sve"
      },
      "id": "vFwrDq8E6Sve",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1POyr7q61CT"
      },
      "id": "a1POyr7q61CT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}