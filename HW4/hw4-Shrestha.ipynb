{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fac98569",
      "metadata": {
        "id": "fac98569"
      },
      "source": [
        "# Homework 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e31c6e87",
      "metadata": {
        "id": "e31c6e87"
      },
      "source": [
        "## 2. Recurrent Network (NN) Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fd76527a",
      "metadata": {
        "id": "fd76527a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"UCI HAR Dataset.zip\"\n",
        "!unzip \"/content/human+activity+recognition+using+smartphones.zip\""
      ],
      "metadata": {
        "id": "lGwu1z0rWOwH",
        "outputId": "817c08fe-ca70-4165-fe4d-42733b22eada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lGwu1z0rWOwH",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCI HAR Dataset.zip\n",
            "replace UCI HAR Dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/UCI HAR Dataset/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace UCI HAR Dataset/activity_labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/UCI HAR Dataset/._activity_labels.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace UCI HAR Dataset/features.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace __MACOSX/UCI HAR Dataset/._features.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/UCI HAR Dataset/._features.txt  \n",
            "  inflating: UCI HAR Dataset/features_info.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._features_info.txt  \n",
            "  inflating: UCI HAR Dataset/README.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._README.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._Inertial Signals  \n",
            "  inflating: UCI HAR Dataset/test/subject_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._subject_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/X_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._X_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._test  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._Inertial Signals  \n",
            "  inflating: UCI HAR Dataset/train/subject_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._subject_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/X_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._X_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._train  \n",
            "  inflating: __MACOSX/._UCI HAR Dataset  \n",
            "Archive:  /content/human+activity+recognition+using+smartphones.zip\n",
            "warning [/content/human+activity+recognition+using+smartphones.zip]:  1048576 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            "file #1:  bad zipfile offset (local header sig):  1048576\n",
            "  (attempting to re-compensate)\n",
            " extracting: UCI HAR Dataset.names   \n",
            "error: not enough memory for bomb detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "9dda9d3b",
      "metadata": {
        "id": "9dda9d3b"
      },
      "outputs": [],
      "source": [
        "# Helper function to load the raw inertial signals\n",
        "def load_ucihar(data_dir=\"UCI HAR Dataset\", subset=\"train\"):\n",
        "    \"\"\" Loads the UCI HAR data from the Inertial Signals folder. Returns: X: numpy array of shape (num_samples, seq_len, num_signals) y: numpy array of labels (0-indexed) \"\"\"\n",
        "    # The nine signal types available in the dataset\n",
        "    signal_types = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\" ]\n",
        "    signals = []\n",
        "    # Each signal file is located in {data_dir}/{subset}/Inertial Signals/\n",
        "    for signal in signal_types:\n",
        "        filename = os.path.join(data_dir, subset, \"Inertial Signals\", f\"{signal}_{ subset}.txt\")\n",
        "        # Each file has shape (num_samples, 128)\n",
        "        data = np.loadtxt(filename) # Add a new axis so that we can later stack to shape (num_samples, 128, num_signals)\n",
        "        signals.append(data[..., np.newaxis]) # Stack along the last dimension to form (num_samples, 128, 9)\n",
        "    X = np.concatenate(signals, axis=2) # Load labels from y_{subset}.txt; labels in the dataset are 1-indexed, so subtract 1.\n",
        "    y_path = os.path.join(data_dir, subset, f\"y_{subset}.txt\")\n",
        "    y = np.loadtxt(y_path).astype(int)- 1\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a PyTorch Dataset for UCI HAR\n",
        "class UCIHARDataset(Dataset):\n",
        "  def __init__(self, data_dir=\"UCI HAR Dataset\", subset=\"train\"):\n",
        "    self.X, self.y = load_ucihar(data_dir, subset)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "    label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "    return sample, label"
      ],
      "metadata": {
        "id": "8RwciBLNWICP"
      },
      "id": "8RwciBLNWICP",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "default_input_dim = 9\n",
        "default_feature_dim = 16\n",
        "default_num_classes = 6\n",
        "\n",
        "# define model\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first= True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x. device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        logit = self.fc(out[:,-1, :])\n",
        "        prob = nn.functional.softmax(logit, dim=1)\n",
        "        return prob, logit\n",
        ""
      ],
      "metadata": {
        "id": "guVfjfwaXHeP"
      },
      "id": "guVfjfwaXHeP",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, batch_size, lr = 10, 16, 0.001\n",
        "\n",
        "# create train, test dataset\n",
        "train_dataset = UCIHARDataset(subset=\"train\")\n",
        "test_dataset = UCIHARDataset(subset=\"test\")\n",
        "\n",
        "#    create train, test loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle= True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle= False)\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=1, num_classes=6)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "U48stUVIvXUx"
      },
      "id": "U48stUVIvXUx",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
        "    # start training\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            activity_prob, activity_logit = model(inputs)\n",
        "            loss = criterion(activity_logit, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on test data\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            activity_prob, activity_logit = model(inputs)\n",
        "            _, predicted = torch.max(activity_logit.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "6xMOnYKhvlxz"
      },
      "id": "6xMOnYKhvlxz",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "N-KVt_hXwpBn",
        "outputId": "f6d4b8d2-23ff-45c1-cec8-b8d60ed9cc10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N-KVt_hXwpBn",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim = 64\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=feature_dim, num_layers=1, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "_8ctji2-y-nk",
        "outputId": "083e544b-a81b-4d5e-9a26-cbc6cebb3f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_8ctji2-y-nk",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dim = 16\n",
        "num_layers = 2\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=feature_dim, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "LzHwO4axy82g",
        "outputId": "0e5b9dfc-3b05-44ac-e08f-2af359cb980f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LzHwO4axy82g",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 3\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "a5XrGk4C1566"
      },
      "id": "a5XrGk4C1566",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = RNNClassifier(input_size=9, hidden_size=16, num_layers=num_layers, num_classes=6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "r9ep-rEO16uj"
      },
      "id": "r9ep-rEO16uj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace RNN with LSTM (feature_dim=16, num_layers=1)\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        logit = self.fc(out[:, -1, :])\n",
        "        prob = nn.functional.softmax(logit, dim=1)\n",
        "        return prob, logit\n",
        "\n",
        "feature_dim = 16\n",
        "num_layers = 1\n",
        "input_size = 9\n",
        "num_classes = 6\n",
        "\n",
        "# create model, loss criterion, optimizer\n",
        "model = LSTMClassifier(input_size=input_size, hidden_size=feature_dim, num_layers=num_layers, num_classes=num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_model_and_test(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "w90xgqUg2FEu"
      },
      "id": "w90xgqUg2FEu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}